
# ðŸ  House Price Prediction Using California Housing Dataset

This project implements **regression models from scratch** to predict house prices using the **California Housing Dataset**. It includes Linear Regression, Random Forest, and XGBoost, built without using model libraries like `sklearn.linear_model` or `xgboost`.

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py               # Main script to run models and compare performance
â”œâ”€â”€ README.md             # Project description and instructions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â””â”€â”€ xgboost.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py        # RMSE and RÂ² implementations
â”‚   â””â”€â”€ visualizations.py # Feature importance plots
```

---

## ðŸš€ How to Run the Project

### 1. Clone the repository

```bash
git clone https://github.com/your-username/house-price-prediction-scratch.git
cd house-price-prediction-scratch
```

### 2. Install dependencies

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

### 3. Run the main script

```bash
python main.py
```

---

## ðŸ”„ Steps Followed

### 1. **Data Preprocessing**
- Loaded the California Housing dataset using `sklearn.datasets`.
- Normalized all numerical features.
- Converted data to NumPy arrays for model compatibility.

### 2. **Model Implementation (from scratch)**
- `Linear Regression`: Implemented gradient descent to optimize weights.
- `Random Forest`: Built multiple decision trees using bootstrap samples.
- `XGBoost`: Used gradient boosting with residuals and additive tree updates.

### 3. **Model Evaluation**
Used the following metrics:
- **RMSE** (Root Mean Squared Error)
- **RÂ² Score**

### 4. **Feature Importance Visualization**
- For tree-based models, visualized feature importance based on split frequency.

---

## ðŸ“Š Observations

| Model             | RMSE     | RÂ² Score |
|------------------|----------|----------|
| Linear Regression| ~0.75    | ~0.58    |
| Random Forest     | ~0.49    | ~0.82    |
| XGBoost           | ~0.46    | ~0.85    |

> ðŸš¨ *Note: Values may vary slightly due to random sampling.*

- Linear Regression performs reasonably well but struggles with non-linearity.
- Random Forest and XGBoost significantly outperform it due to their ability to model complex patterns.
- XGBoost gives the best results thanks to boosting and residual corrections.

---

## ðŸ“ˆ Visualizations

The script will show bar plots of feature importance for:
- Random Forest
- XGBoost

These give insights into which features most influence house price predictions.

---

## ðŸ“Œ To-Do / Future Work
- Add support for categorical features (if dataset extended)
- Optimize tree-based models with pruning and early stopping
- Add unit tests and refactor into modules

---

## ðŸ“§ Contact

Feel free to reach out if you have questions or suggestions!

ðŸ“® **Your Name**  
ðŸ”— [GitHub](https://github.com/your-username) | ðŸ“¬ your.email@example.com

---
